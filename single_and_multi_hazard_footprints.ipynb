{
 "cells":  [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b04faa-fb23-4761-8d2c-92ba6c3e1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import cftime as cftime\n",
    "import fiona\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import mapping\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3974d80-4345-4699-9f05-c4d1b22d8d54",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180572fb-757d-4df1-9ca1-f7238c7a32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(hazard_name,variable_name):\n",
    "  # This function takes the input climate data for each hazard in the format of netcdf files. \n",
    "  # Two type of files are required: the netcdf of values above thresholds and the original climate variable\n",
    "  # It loads the input data into a dataset with xarray and merges the daily climate data with the masks into a single dataset\n",
    "\n",
    "  # Input variables:\n",
    "  # model: corresponds to the name of the climate model or the reanalysis; it is used as part of the name of the file path\n",
    "  # percentile: corresponds to the percentile applied in the threshold calculation; it is used as part of the name of the file path\n",
    "  # variable: name of the climate variable; it is used as part of the name of the file path\n",
    "  # mask name: name of the file mask (events above threshold); it is used as part of the name of the file path\n",
    "  # values name: name of the file of the daily climate variable; it is used as part of the name of the file path\n",
    "  # var name: name of the climate variable; it is used to access the variable in the xarray dataset\n",
    "  # start_year: first year of the data; it is used as part of the name of the file path\n",
    "\n",
    "  # Output variable:\n",
    "  # an xarray dataset with updated names for the climate variable and the mask\n",
    "  path_name='INPUT_data/'+hazard_name+'_mask.nc'\n",
    "  path_values='INPUT_data/'+hazard_name+'_data.nc'\n",
    "  var_mask=xr.open_dataset(str(path_name))\n",
    "  var_values=xr.open_dataset(str(path_values))\n",
    "  var_all=var_values.merge(var_mask)\n",
    "  error_code='Full_dataset'\n",
    "  return(var_all)\n",
    "\n",
    "\n",
    "def day_sequence(var_df,start_date):\n",
    "  #This functions creates a list of days from 1 to the end of the time period, useful to calculate distance in the time dimension\n",
    "  # Slice the data to select only a timeframe, provide the start year (e.g., January 1, 2023)\n",
    "  start_date = start_date.date()\n",
    "  #print('the reference date is',start_date)\n",
    "  list_of_days=[]\n",
    "  for i in var_df.time:\n",
    "    #print('the day counted is',i)\n",
    "    time_difference = i - start_date\n",
    "    #print('the time_difference is',time_difference)\n",
    "    days_elapsed = time_difference.days\n",
    "    #print('the days elapsed are',days_elapsed)\n",
    "    days_elapsed_integer = int(days_elapsed)\n",
    "    list_of_days.append(days_elapsed_integer)\n",
    "  return(list_of_days)\n",
    "  #print('the final integer is',days_elapsed_integer)\n",
    "\n",
    "#This functions creates the days sequence with the progressive list of days produced in the day_sequence function\n",
    "def time_preprocessing(var_ds,year_start='2018',year_end='2022'):#add a column with a progressive list of days\n",
    "  var_ds=var_ds.sel(time=slice(str(year_start),str(year_end)))\n",
    "  var_df=var_ds.to_dataframe().dropna().reset_index()\n",
    "  var_df['time']=pd.to_datetime(var_df['time']).dt.date\n",
    "  day_1=pd.to_datetime(str(year_start)+'-1-1')\n",
    "  days_list=day_sequence(var_df,day_1)\n",
    "  var_df['days']=days_list\n",
    "  return(var_df)\n",
    "\n",
    "#This function numbers cells in a progressive way, in order to have a unique reference ID for each cell\n",
    "def cell_preprocessing(var_df):#add a column with a progressive list of cells and return also the df with the pairing\n",
    "  var_df['cell'] = var_df.groupby(['lon', 'lat']).ngroup() + 1\n",
    "  df_mapping=var_df[['cell','lon','lat']].drop_duplicates(subset='cell')\n",
    "  return var_df,df_mapping\n",
    "# to recrete the orginal coordinates: merged_df = prova.merge(wind_coords, on='cell', how='left')\n",
    "\n",
    "#This function filters the mask file with minimum empirical thresholds. E.g., 20 mm/day fo precipitation, 13.9 m/s for wind\n",
    "def filtering_thresholds(var_df,var_name,option):\n",
    "  if option=='_NO_F_':\n",
    "      return(var_df)\n",
    "  #print(var_name)\n",
    "  if 'WIND_SPEED' in var_name:\n",
    "    threshold=13.9 #more than Gale force 7\n",
    "    var_df[var_name]= var_df.apply(lambda row: 0 if row['WIND_SPEED'] < threshold else row[var_name], axis=1)\n",
    "  if 'TOT_PREC' in var_name:\n",
    "    threshold=20\n",
    "    var_df[var_name]= var_df.apply(lambda row: 0 if row['TOT_PREC'] < threshold else row[var_name], axis=1)\n",
    "  if 'T_2M' in var_name:\n",
    "    threshold=273.15 # more than 0 ÂºC\n",
    "    var_df[var_name]= var_df.apply(lambda row: 0 if row['T_2M'] < threshold else row[var_name], axis=1)\n",
    "  return(var_df)\n",
    "\n",
    "#This function sets the distance between two cells in x and y dimension to 1 for the CMCC dataset\n",
    "def spatio_temporal_weight(cluster_df,lon_factor=35.335689,lat_factor=50.3778338): #these factors are specific for the dimension in degree of the remapped dataset\n",
    "  cluster_df['new_lon']=cluster_df['lon']*lon_factor\n",
    "  cluster_df['new_lat']=cluster_df['lat']*lat_factor\n",
    "  min_lat=cluster_df['new_lat'].min()\n",
    "  min_lon=cluster_df['new_lon'].min()\n",
    "  cluster_df['new_lon']=cluster_df['new_lon']-min_lon\n",
    "  cluster_df['new_lat']=cluster_df['new_lat']-min_lat\n",
    "  return(cluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9168ed5-999e-4f08-b9b4-cc56410a59c5",
   "metadata": {},
   "source": [
    "### Clustering single hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d22f56-5085-4a94-aa5c-d42e3c5ea19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the main DBSCAN function, and takes as input: \n",
    "#the mask data, \n",
    "#the variable name, \n",
    "#the multiplicative weight to be associated to days (increasing the distance between 2 days), \n",
    "#the epsilon parameter, \n",
    "#the min_element parameters and \n",
    "#the option to have a \"_YES_F_' or \"_NO_F_' data) choosing whether to filter or not climate inputs with empirical values\n",
    "\n",
    "def clustering_application(var_df, var_name, day_weight, epsilon, min_elements, option):\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import numpy as np\n",
    "\n",
    "    var_name_thres=var_name+'_MASK'\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    var_df = filtering_thresholds(var_df, var_name_thres, option)\n",
    "    grid_df=var_df.groupby(['lon','lat'])[var_name_thres].max().reset_index()\n",
    "    grid_df.drop(columns=[var_name_thres],inplace=True)\n",
    "\n",
    "    # Select relevant data\n",
    "    cluster_df = var_df[var_df[var_name_thres] == 1]\n",
    "    crop_df = var_df[var_df[var_name_thres] == 1]\n",
    "\n",
    "    # Apply weights and prepare data for clustering\n",
    "    cluster_df['days'] = (cluster_df['days'] + 1) * day_weight\n",
    "    cluster_df = cluster_df[['lat', 'lon', 'days']]\n",
    "    cluster_df = spatio_temporal_weight(cluster_df)\n",
    "    cluster_df = cluster_df[['new_lat', 'new_lon', 'days']]\n",
    "    X = cluster_df.to_numpy()\n",
    "\n",
    "    # Fit DBSCAN\n",
    "    dbscan_cluster_model = DBSCAN(eps=epsilon, min_samples=min_elements).fit(X)\n",
    "    labels = dbscan_cluster_model.labels_\n",
    "    #print('the labels are',labels)\n",
    "\n",
    "    # Identify core points\n",
    "    core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "    core_samples_mask[dbscan_cluster_model.core_sample_indices_] = True\n",
    "    #print('the core sample is',core_samples_mask)\n",
    "\n",
    "    # Classify points\n",
    "    crop_df['clusters'] = labels\n",
    "    crop_df['core'] = core_samples_mask\n",
    "    #print(crop_df.columns)\n",
    "\n",
    "    return(crop_df,grid_df)\n",
    "\n",
    "def single_hazard_clusters_update(df_single_cluster):\n",
    "  # This function calculates the characteristics of the clusters and applies a spatial merge\n",
    "  # Without the spatial merge, each day can have different clusters\n",
    "  # given the extremely local application in Veneto, different clusters are merged together if they happen on the same day\n",
    "  # in case of wider areas of application, this has to be removed to investigate spatially co-occurring hazards   \n",
    "  df_char_temp=cluster_characteristics(df_single_cluster,'clusters')#calculates start day, end day, duration, extension of the original clusters\n",
    "  df_char_temp=df_char_temp[df_char_temp['clusters']!=-1]\n",
    "  df_preproc=merging_clusters(df_char_temp,0) #merging cluster function. delta=0 implies only a spatial merge\n",
    "  df_single_cluster=df_single_cluster[df_single_cluster['clusters']!=-1]\n",
    "  df_new_char=clustering_new_indexing(df_char_temp,df_preproc)#reindex clusters atfer merging\n",
    "  df_merging=df_new_char[['clusters','new_clusters']]\n",
    "  df_final_single_cluster=df_single_cluster.merge(df_merging,on='clusters')\n",
    "  df_final_chars=cluster_characteristics(df_final_single_cluster,'new_clusters')#calculates new hazard characteristics after merge\n",
    "  return(df_final_single_cluster,df_final_chars)\n",
    "\n",
    "#Calculating the characteristics fo the clusters, to check for duration, intensity and fragmentation of the analysis. \n",
    "# Used later to merge events happening in the same day\n",
    "\n",
    "def cluster_characteristics(df_cluster,cluster_col):\n",
    "    df_cluster['time'] = pd.to_datetime(df_cluster['time'])\n",
    "    # Get min and max time per cluster\n",
    "    day_min = df_cluster.groupby(cluster_col)['time'].min().rename('day_start')\n",
    "    day_max = df_cluster.groupby(cluster_col)['time'].max().rename('day_end')\n",
    "    # Compute daily cell counts\n",
    "    extension = df_cluster.groupby([cluster_col, 'time'])['lat'].count().rename('cells_per_day').reset_index() \n",
    "    # Aggregating cell count statistics\n",
    "    extension_stats = extension.groupby(cluster_col)['cells_per_day'].agg(\n",
    "        extension_mean='mean',\n",
    "        extension_min='min',\n",
    "        extension_max='max'\n",
    "    )\n",
    "    # Total elements per cluster\n",
    "    total_cluster = df_cluster.groupby(cluster_col)['lat'].count().rename('total_elements')\n",
    "    # Merging all statistics\n",
    "    cluster_chars = (\n",
    "        pd.concat([day_min, day_max, extension_stats, total_cluster], axis=1)\n",
    "        .reset_index()\n",
    "    )\n",
    "    # Compute duration in days\n",
    "    cluster_chars['duration'] = ((cluster_chars['day_end'] - cluster_chars['day_start']).dt.total_seconds() + 86400) / (24 * 60 * 60)\n",
    "    # Compute date difference between consecutive clusters\n",
    "    cluster_chars['date_difference'] = cluster_chars['day_start'].shift(-1) - cluster_chars['day_end']\n",
    "\n",
    "    return cluster_chars\n",
    "\n",
    "def merging_clusters(df,delta):\n",
    "    #delta is the number of days in which i want to merge clusters. \n",
    "    #If delta is set to 0, the function merges clusters only spatially.\n",
    "    merged_clusters = pd.DataFrame(columns=['clusters', 'day_start', 'day_end', 'new_order'])# Create an empty DataFrame to store the results\n",
    "    new_order_counter = 1 # Initialize the new_order counter\n",
    "    dfs = []# List to collect DataFrames for concatenation\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if the current row overlaps with any existing merged clusters\n",
    "        overlapping_clusters = merged_clusters[\n",
    "            (merged_clusters['day_start'] <= row['day_end'] + pd.Timedelta(days=delta)) & \n",
    "            (merged_clusters['day_end'] >= row['day_start'] + pd.Timedelta(days=-delta))\n",
    "        ]\n",
    "        if overlapping_clusters.empty:\n",
    "            # If no overlapping clusters, create a new merged cluster\n",
    "            new_row = pd.DataFrame([{\n",
    "                'clusters': row['clusters'],\n",
    "                'day_start': row['day_start'],\n",
    "                'day_end': row['day_end'],\n",
    "                'new_order': new_order_counter\n",
    "            }])\n",
    "            dfs.append(new_row)\n",
    "            new_order_counter += 1\n",
    "        else:\n",
    "            # If overlapping clusters, merge them and update the new_order for all involved clusters\n",
    "            merged_clusters = merged_clusters[~merged_clusters['clusters'].isin(overlapping_clusters['clusters'])]\n",
    "            new_row = pd.DataFrame([{\n",
    "                'clusters': ', '.join(map(str, overlapping_clusters['clusters'].tolist() + [row['clusters']])),\n",
    "                'day_start': min(overlapping_clusters['day_start'].min(), row['day_start']),\n",
    "                'day_end': max(overlapping_clusters['day_end'].max(), row['day_end']),\n",
    "                'new_order': new_order_counter\n",
    "            }])\n",
    "            dfs.append(new_row)\n",
    "            new_order_counter += 1\n",
    "\n",
    "        # Update merged_clusters DataFrame after processing each row\n",
    "        if dfs:\n",
    "            merged_clusters = pd.concat([merged_clusters] + dfs, ignore_index=True)\n",
    "            dfs.clear()\n",
    "\n",
    "    return merged_clusters\n",
    "\n",
    "def clustering_new_indexing(df_char,df_processed):\n",
    "  df_processed['new_cluster_label']=df_processed.index\n",
    "  list_of_vals=[]\n",
    "  for index,row in df_processed.iterrows():\n",
    "    if type(row['clusters']) != int:\n",
    "      cluster_values = row['clusters'].split(', ')\n",
    "    else:\n",
    "      cluster_values=[int(row['clusters'])]\n",
    "    length=len(cluster_values)\n",
    "    for i in range(0,length):\n",
    "      list_of_vals.append(row['new_cluster_label'])\n",
    "  df_char['new_clusters']=list_of_vals\n",
    "  return(df_char)\n",
    "\n",
    "\n",
    "def single_hazard_cluster(hazard,variable,weight,epsilon,minimum_points,option):\n",
    "  var_ds=load_dataset(hazard,variable)\n",
    "  var_ds2=time_preprocessing(var_ds)\n",
    "  var_ds2,var_coords=cell_preprocessing(var_ds2)\n",
    "  result_df,grid_df=clustering_application(var_ds2,variable,weight,epsilon,minimum_points,option)\n",
    "  new_result,grid_df2=single_hazard_clusters_update(result_df)\n",
    "  new_result.drop(columns='clusters',inplace=True)\n",
    "  return(new_result,grid_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ee771-2f7f-48e6-a927-79f6933027dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatwave,grid_heatwave=single_hazard_cluster('temp','T_2M',1,5,20,'_YES_F_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cdd2a-b517-4811-b674-6a8f57e6f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation,grid_precipitation=single_hazard_cluster('prec','TOT_PREC',10,11,100,'_YES_F_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0fbb9-59ea-4563-8f55-10da5bde87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought,grid_drought=single_hazard_cluster('drought','SPI_12',0.3,5,500,'_YES_F_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59669f3-7b2d-47bc-a643-855f6c9425d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind,grid_wind=single_hazard_cluster('wind','WIND_SPEED',10,11,100,'_YES_F_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19bbb1-b2f0-4d6f-ab53-e6d5dd698b2c",
   "metadata": {},
   "source": [
    "### Visualising single hazard clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6d26f-6b25-492f-af9c-48e2d84cefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_clusters_intensity(df, value_column,hazard, start_date=None, end_date=None, cmap=\"magma\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.colors as mcolors\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    import matplotlib.dates as mdates\n",
    "    \"\"\"\n",
    "    Plots a 3D scatter plot of longitude, latitude, and time, \n",
    "    with points colored according to the values in a given column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing 'lon', 'lat', 'time', and the specified value column.\n",
    "        value_column (str): The column to use for coloring the points.\n",
    "        start_date (str, optional): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str, optional): End date in 'YYYY-MM-DD' format.\n",
    "        cmap (str, optional): Colormap to use (default is \"viridis\").\n",
    "    \"\"\"\n",
    "    # Convert 'time' column to datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df=df[df['new_clusters']!=-1]\n",
    "\n",
    "    # Apply date filter if provided\n",
    "    if start_date:\n",
    "        df = df[df[\"time\"] >= pd.to_datetime(start_date)]\n",
    "    if end_date:\n",
    "        df = df[df[\"time\"] <= pd.to_datetime(end_date)]\n",
    "\n",
    "    # Get min and max values for normalization\n",
    "    vmin = df[value_column].min()\n",
    "    vmax = df[value_column].max()\n",
    "\n",
    "    # Define colormap\n",
    "    \n",
    "    if hazard!='Drought':\n",
    "        norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "        cmap = plt.get_cmap(cmap)\n",
    "    else:\n",
    "        norm = mcolors.Normalize(vmin=vmin, vmax=0)\n",
    "        cmap = plt.get_cmap(cmap+\"_r\")\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Scatter plot\n",
    "    sc = ax.scatter(df[\"lon\"], df[\"lat\"], df[\"time\"].map(mdates.date2num), \n",
    "                    c=df[value_column], cmap=cmap, norm=norm, alpha=0.7)\n",
    "\n",
    "    # Labels and formatting\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlabel(\"Longitude\", labelpad=20)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylabel(\"Latitude\", labelpad=20)\n",
    "    #ax.set_zlabel(\"Time\", labelpad=10)\n",
    "    ax.set_title(f\"3D Multi-Hazard clusters - {hazard}\", pad=20)\n",
    "    ax.view_init(elev=20, azim=60)\n",
    "\n",
    "    # Format Z-axis to display real dates\n",
    "    ax.zaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.zaxis.set_major_locator(mdates.AutoDateLocator(minticks=3, maxticks=6))\n",
    "    plt.setp(ax.get_zticklabels(), rotation=15, ha=\"right\")\n",
    "    ax.tick_params(axis='z', pad=35)  # Add padding to the Z-axis tick labels\n",
    "    ax.zaxis.labelpad = 10  # Add padding to the Z-axis label\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(sc, ax=ax, pad=0.1)\n",
    "    cbar.set_label(value_column)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_3d_clusters(df, hazard,cluster_column='new_clusters',start_date=None, end_date=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.colors as mcolors\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    import matplotlib.dates as mdates\n",
    "    \"\"\"\n",
    "    Plots a 3D scatter plot of longitude, latitude, and time, \n",
    "    with points colored according to the 'clusters' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing 'lon', 'lat', 'time', and 'clusters'.\n",
    "        start_date (str, optional): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str, optional): End date in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    # Convert 'time' column to datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df=df[df['new_clusters']!=-1]\n",
    "\n",
    "    # Apply date filter if provided\n",
    "    if start_date:\n",
    "        df = df[df[\"time\"] >= pd.to_datetime(start_date)]\n",
    "    if end_date:\n",
    "        df = df[df[\"time\"] <= pd.to_datetime(end_date)]\n",
    "\n",
    "    # Define colors for clusters\n",
    "    unique_clusters = df[cluster_column].unique()\n",
    "    colors = plt.cm.get_cmap(\"tab10\", len(unique_clusters))  # Get distinct colors\n",
    "    cluster_color_map = {c: colors(i) for i, c in enumerate(unique_clusters)}\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    #ax = fig.add_axes([0.6, 0.3, 0.35, 0.35], projection='3d')\n",
    "\n",
    "    # Scatter plot\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_df = df[df[cluster_column] == cluster]\n",
    "        ax.scatter(cluster_df[\"lon\"], cluster_df[\"lat\"], cluster_df[\"time\"].map(mdates.date2num), \n",
    "                   color=cluster_color_map[cluster], label=f\"Cluster {cluster}\", alpha=0.4)\n",
    "\n",
    "    # Labels and formatting\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlabel(\"Longitude\", labelpad=20)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylabel(\"Latitude\", labelpad=20)\n",
    "    #ax.set_zlabel(\"Time\", labelpad=10)\n",
    "    ax.set_title(\"3D Multi-Hazard Clustering - \"+hazard, pad=20)\n",
    "    #ax.set_title(\"3D Multi-Hazard Clustering\")\n",
    "    ax.view_init(elev=20, azim=60)\n",
    "\n",
    "    \n",
    "    # Format Z-axis to display real dates\n",
    "    ax.zaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    #ax.zaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.zaxis.set_major_locator(mdates.AutoDateLocator(minticks=3, maxticks=6))\n",
    "    # Rotate Z-axis labels to prevent overlap\n",
    "    #ax.zaxis.labelpad = 25\n",
    "    plt.setp(ax.get_zticklabels(), rotation=15, ha=\"right\")\n",
    "    ax.tick_params(axis='z', pad=35)  # Add padding to the Z-axis tick labels\n",
    "    ax.zaxis.labelpad = 10  # Add padding to the Z-axis label\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61900100-3046-4c6a-8066-958a648153db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters(heatwave, 'Heatwaves',start_date=\"2018-01-01\", end_date=\"2022-10-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cf82a-9f3f-4153-ae2d-0283cd4b1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters_intensity(heatwave,'T_2M','Heatwaves',start_date=\"2018-01-01\", end_date=\"2022-10-31\",cmap='seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c1f1c-fb0e-4271-92e6-1857a732e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters(precipitation, 'Precipitation','new_clusters',start_date=\"2018-01-01\", end_date=\"2022-08-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebf1f1-3d21-4d6f-ae87-fda85d7c1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters_intensity(precipitation, 'TOT_PREC','Precipitation',start_date=\"2022-08-01\", end_date=\"2022-08-31\",cmap='seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50c228-8834-445b-97f9-a6043db95f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters(drought, 'Drought','new_clusters',start_date=\"2018-01-01\", end_date=\"2022-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7edb0d-1cd6-4c23-8ca6-d3aeddd85bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters_intensity(drought, 'SPI_12','Drought',start_date=\"2022-01-01\", end_date=\"2022-12-31\",cmap='seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f910c4-772a-42c2-9d7e-5728546e1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters(wind, 'Wind','new_clusters',start_date=\"2022-02-01\", end_date=\"2022-02-28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcc9d4-b745-4e01-86a4-725599a071ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_clusters_intensity(wind, 'WIND_SPEED','Wind',start_date=\"2022-02-01\", end_date=\"2022-02-28\",cmap='seismic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1cead-94c0-4559-bab1-708be840d05b",
   "metadata": {},
   "source": [
    "### Multi-hazard events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47856f-4e03-4c32-93e8-81594c5d6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_events_creation(df):\n",
    "    df['prec_wind'] = np.where(df['WIND_SPEED'].isnull() | df['TOT_PREC'].isnull(),\n",
    "                           np.nan,\n",
    "                           'w_' + df['wind_clusters'].astype(str) + 'p_' + df['prec_clusters'].astype(str) + '_'+df['year'].astype(str))\n",
    "    df['prec_drought'] = np.where(df['SPI_12'].isnull() | df['TOT_PREC'].isnull(),\n",
    "                           np.nan,\n",
    "                           'd_' + df['drought_clusters'].astype(str) + 'p_' + df['prec_clusters'].astype(str) + '_'+df['year'].astype(str))\n",
    "    df['prec_heat'] = np.where(df['T_2M'].isnull() | df['TOT_PREC'].isnull(),\n",
    "                           np.nan,\n",
    "                           'h_' + df['temp_clusters'].astype(str) + 'p_' + df['prec_clusters'].astype(str) +'_'+ df['year'].astype(str))\n",
    "    df['drought_wind'] = np.where(df['WIND_SPEED'].isnull() | df['SPI_12'].isnull(),\n",
    "                           np.nan,\n",
    "                           'w_' + df['wind_clusters'].astype(str) + 'd_' + df['drought_clusters'].astype(str) + '_'+df['year'].astype(str))\n",
    "    df['drought_heat'] = np.where(df['SPI_12'].isnull() | df['T_2M'].isnull(),\n",
    "                           np.nan,\n",
    "                           'd_' + df['drought_clusters'].astype(str) + 'h_' + df['temp_clusters'].astype(str) + '_'+df['year'].astype(str))\n",
    "    df['heat_wind'] = np.where(df['WIND_SPEED'].isnull() | df['T_2M'].isnull(),\n",
    "                           np.nan,\n",
    "                           'w_' + df['wind_clusters'].astype(str) + 'h_' + df['temp_clusters'].astype(str) + '_'+df['year'].astype(str))\n",
    "    df['prec_wind_heat'] = np.where(df['WIND_SPEED'].isnull() | df['TOT_PREC'].isnull() | df['T_2M'].isnull(),\n",
    "                           np.nan,\n",
    "                           'w_' + df['wind_clusters'].astype(str) + 'p_' + df['prec_clusters'].astype(str) + 'h_' + df['temp_clusters'].astype(str)+'_'+df['year'].astype(str))\n",
    "    df['prec_wind_drought'] = np.where(df['WIND_SPEED'].isnull() | df['TOT_PREC'].isnull() | df['SPI_12'].isnull(),\n",
    "                           np.nan,\n",
    "                           'w_' + df['wind_clusters'].astype(str) + 'p_' + df['prec_clusters'].astype(str) + 'd_' + df['drought_clusters'].astype(str)+'_'+df['year'].astype(str))\n",
    "    df['prec_drought_heat'] = np.where(df['WIND_SPEED'].isnull() | df['SPI_12'].isnull() | df['T_2M'].isnull(),\n",
    "                           np.nan,\n",
    "                           'w_' + df['wind_clusters'].astype(str) + 'd_' + df['drought_clusters'].astype(str) + 'h_' + df['temp_clusters'].astype(str)+'_'+df['year'].astype(str))\n",
    "    df['drought_wind_heat'] = np.where(df['SPI_12'].isnull() | df['TOT_PREC'].isnull() | df['T_2M'].isnull(),\n",
    "                           np.nan,\n",
    "                           'd_' + df['drought_clusters'].astype(str) + 'p_' + df['prec_clusters'].astype(str) + 'h_' + df['temp_clusters'].astype(str)+'_'+df['year'].astype(str))\n",
    "    df['prec_wind_heat_drought'] = np.where(df['WIND_SPEED'].isnull() | df['TOT_PREC'].isnull() | df['T_2M'].isnull() | df['SPI_12'].isnull(),\n",
    "                           np.nan,\n",
    "                           'w_' + df['wind_clusters'].astype(str) + 'p_' + df['prec_clusters'].astype(str) + 'h_' + df['temp_clusters'].astype(str)+'d_' + df['drought_clusters'].astype(str)+'_'+df['year'].astype(str))\n",
    "    return(df)\n",
    "\n",
    "def merge_single_hazards(df_prec,df_wind,df_heat,df_drought):\n",
    "    df_prec.rename(columns={'new_clusters':'prec_clusters'},inplace=True)\n",
    "    df_wind.rename(columns={'new_clusters':'wind_clusters'},inplace=True)\n",
    "    df_drought.rename(columns={'new_clusters':'drought_clusters'},inplace=True)\n",
    "    df_heat.rename(columns={'new_clusters':'temp_clusters'},inplace=True)\n",
    "    df_prec.drop(columns=['days','cell','core'],inplace=True)\n",
    "    df_wind.drop(columns=['days','cell','core'],inplace=True)\n",
    "    df_drought.drop(columns=['days','cell','core'],inplace=True)\n",
    "    df_heat.drop(columns=['days','cell','core'],inplace=True)\n",
    "    df_merged=df_prec.merge(df_wind,on=['time','lon','lat'],how='outer')\n",
    "    df_merged=df_merged.merge(df_heat,on=['time','lon','lat'],how='outer')\n",
    "    df_merged=df_merged.merge(df_drought,on=['time','lon','lat'],how='outer')\n",
    "    df_merged['year']=df_merged['time'].dt.year\n",
    "    #print('the merged dataset columns are: ',df_merged.columns)\n",
    "    return(df_merged)\n",
    "\n",
    "def compound_events_flattening(df):\n",
    "    df2 = df.drop(columns=['time', 'year', 'TOT_PREC', 'T_2M', 'SPI_12', 'WIND_SPEED'])\n",
    "    flattened_multi_hazards = df2.groupby(['lon', 'lat']).count().reset_index()\n",
    "\n",
    "    # Extract single and multi-hazard columns\n",
    "    single_hazards = flattened_multi_hazards[['lon', 'lat', 'prec_clusters', 'wind_clusters', 'temp_clusters', 'drought_clusters']]\n",
    "    multi_hazards_only = flattened_multi_hazards[['lon', 'lat', 'prec_wind', 'prec_drought', 'prec_heat', 'drought_wind', \n",
    "                                                  'drought_heat', 'heat_wind', 'prec_wind_heat', 'prec_wind_drought', \n",
    "                                                  'prec_drought_heat', 'drought_wind_heat', 'prec_wind_heat_drought']]\n",
    "\n",
    "    # Assign max column name, but set 'no_hazard' if max value is 0\n",
    "    single_hazards['class'] = single_hazards.iloc[:, 2:].apply(lambda row: row.idxmax() if row.max() > 0 else 'no_hazard', axis=1)\n",
    "    multi_hazards_only['class'] = multi_hazards_only.iloc[:, 2:].apply(lambda row: row.idxmax() if row.max() > 0 else 'no_hazard', axis=1)\n",
    "\n",
    "    return flattened_multi_hazards, single_hazards, multi_hazards_only\n",
    "\n",
    "def multi_hazarding(prec_cluster,wind_cluster,heat_cluster,drought_cluster):\n",
    "    multi_hazard_df=merge_single_hazards(prec_cluster,wind_cluster,heat_cluster,drought_cluster)\n",
    "    #print('the multi-hazard columns are',multi_hazard_df.columns)\n",
    "    compound_events_df=compound_events_creation(multi_hazard_df)\n",
    "    flat_mh,flat_sh,flat_mh_only=compound_events_flattening(compound_events_df)\n",
    "    return(compound_events_df,flat_mh,flat_sh,flat_mh_only)\n",
    "\n",
    "def add_time_lag_rows(df,n,cluster_name,hazard):\n",
    "    \"\"\"\n",
    "    Adds n rows with a time lag to the DataFrame. Each new row has the same values \n",
    "    except for the date, which is incremented by i (1 to n).\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    n (int): The number of new rows to create for each row in the original DataFrame.\n",
    "    cluster_name (string): name of the column with single hazard clusters\n",
    "    hazard (string): name of the hazard (e.g., drought, heat, prec, wind)\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with additional rows.\n",
    "    \"\"\"\n",
    "    # Empty list to hold the new rows\n",
    "    new_rows = []\n",
    "    df['time']=pd.to_datetime(df['time'])\n",
    "    if 'compound_'+hazard+'_check' in df.columns:\n",
    "        df.drop(columns=['compound_'+hazard+'_check'],inplace=True)\n",
    "    \n",
    "    # Iterate over each row in the original DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        for i in range(1, n + 1):\n",
    "            # Create a new row with the same values as the original row\n",
    "            new_row = row.copy()\n",
    "            \n",
    "            # Increment the date by i days\n",
    "            new_row['time'] = pd.to_datetime(new_row['time']) + pd.Timedelta(days=i)\n",
    "            \n",
    "            # Append the new row to the new rows list\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    df_new = pd.DataFrame(new_rows)\n",
    "    print('The columns of df_new are: ',df_new.columns)\n",
    "    # Concatenate the original DataFrame with the new DataFrame\n",
    "    df_combined = pd.concat([df, df_new], ignore_index=True)\n",
    "    df_combined['time'] = pd.to_datetime(df_combined['time'])  # Ensure date is in datetime format\n",
    "    print('The size before the dropping of duplicates is',df_combined.shape[0])\n",
    "    print('The columns of df_combined are: ',df_combined.columns)\n",
    "    #print(df_combined)\n",
    "    df_cleaned = df_combined.drop_duplicates(subset=['lat', 'lon', 'time',cluster_name], keep='first') #drop duplicates\n",
    "    print('The columns of df_clean are: ',df_cleaned.columns)\n",
    "    print('The size after the dropping of duplicates is',df_cleaned.shape[0])\n",
    "    df_2=df.copy()\n",
    "    df_2['compound_'+hazard+'_check']=1\n",
    "    df_cleaned=df_cleaned.merge(df_2[['lat','lon','time','compound_'+hazard+'_check']],on=['lat','lon','time'],how='outer')\n",
    "    print('The size after adding of check of compoundness is',df_cleaned.shape[0])\n",
    "    return df_cleaned\n",
    "\n",
    "def consecutive_hazards(df_prec,df_wind,df_heat,df_drought,n):\n",
    "    \"\"\"creates the consecutive event dataset\n",
    "    \"\"\"\n",
    "    prec_lag_days=add_time_lag_rows(df_prec, n,'prec_clusters','prec')\n",
    "    wind_lag_days=add_time_lag_rows(df_wind, n,'wind_clusters','wind')\n",
    "    heat_lag_days=add_time_lag_rows(df_heat, n,'temp_clusters','heat')\n",
    "    drought_lag_days=add_time_lag_rows(df_drought, n,'drought_clusters','drought')\n",
    "    \n",
    "    heat_1days_first=heat_lag_days.groupby(['lon','lat','temp_clusters'])['time'].min().reset_index(name='first_day_temp')\n",
    "    heat_1days_last=heat_lag_days.groupby(['lon','lat','temp_clusters'])['time'].max().reset_index(name='last_day_temp')\n",
    "    prec_1days_first=prec_lag_days.groupby(['lon','lat','prec_clusters'])['time'].min().reset_index(name='first_day_prec')\n",
    "    prec_1days_last=prec_lag_days.groupby(['lon','lat','prec_clusters'])['time'].max().reset_index(name='last_day_prec')\n",
    "    wind_1days_first=wind_lag_days.groupby(['lon','lat','wind_clusters'])['time'].min().reset_index(name='first_day_wind')\n",
    "    wind_1days_last=wind_lag_days.groupby(['lon','lat','wind_clusters'])['time'].max().reset_index(name='last_day_wind')\n",
    "    drought_1days_first=drought_lag_days.groupby(['lon','lat','drought_clusters'])['time'].min().reset_index(name='first_day_drought')\n",
    "    drought_1days_last=drought_lag_days.groupby(['lon','lat','drought_clusters'])['time'].max().reset_index(name='last_day_drought')\n",
    "    \n",
    "    drought_lag_days=drought_lag_days.merge(drought_1days_first,on=['lon','lat','drought_clusters'],how='outer')\n",
    "    drought_lag_days=drought_lag_days.merge(drought_1days_last,on=['lon','lat','drought_clusters'],how='outer')\n",
    "    heat_lag_days=heat_lag_days.merge(heat_1days_first,on=['lon','lat','temp_clusters'],how='outer')\n",
    "    heat_lag_days=heat_lag_days.merge(heat_1days_last,on=['lon','lat','temp_clusters'],how='outer')\n",
    "    prec_lag_days=prec_lag_days.merge(prec_1days_first,on=['lon','lat','prec_clusters'],how='outer')\n",
    "    prec_lag_days=prec_lag_days.merge(prec_1days_last,on=['lon','lat','prec_clusters'],how='outer')\n",
    "    wind_lag_days=wind_lag_days.merge(wind_1days_first,on=['lon','lat','wind_clusters'],how='outer')\n",
    "    wind_lag_days=wind_lag_days.merge(wind_1days_last,on=['lon','lat','wind_clusters'],how='outer')\n",
    "    \n",
    "\n",
    "    consecutive_hazards=heat_lag_days.merge(drought_lag_days,on=['lon','lat','time'],how='outer')\n",
    "    consecutive_hazards=consecutive_hazards.merge(wind_lag_days,on=['lon','lat','time'],how='outer')\n",
    "    consecutive_hazards=consecutive_hazards.merge(prec_lag_days,on=['lon','lat','time'],how='outer')\n",
    "    consecutive_hazards['year']=consecutive_hazards['time'].dt.year\n",
    "    consecutive_hazards=compound_events_creation(consecutive_hazards)\n",
    "    consecutive_hazards=clean_hazard_combinations(consecutive_hazards)\n",
    "    return(consecutive_hazards)\n",
    "\n",
    "def clean_hazard_combinations(df):\n",
    "    \"\"\"Cleans the consecutive datasets from tail events, i.e. events in which the time lag creates a tail of new hazards that do not overlap with other existing events\n",
    "    \"\"\"\n",
    "    hazard_pairs = {\n",
    "        'prec_wind': ('last_day_prec', 'last_day_wind'),\n",
    "        'prec_drought': ('last_day_prec', 'last_day_drought'),\n",
    "        'prec_heat': ('last_day_prec', 'last_day_temp'),\n",
    "        'drought_wind': ('last_day_drought', 'last_day_wind'),\n",
    "        'drought_heat': ('last_day_drought', 'last_day_temp'),\n",
    "        'heat_wind': ('last_day_temp', 'last_day_wind'),\n",
    "        'prec_wind_heat': ('last_day_prec', 'last_day_wind', 'last_day_temp'),\n",
    "        'prec_wind_drought': ('last_day_prec', 'last_day_wind', 'last_day_drought'),\n",
    "        'prec_drought_heat': ('last_day_prec', 'last_day_drought', 'last_day_temp'),\n",
    "        'drought_wind_heat': ('last_day_drought', 'last_day_wind', 'last_day_temp'),\n",
    "        'prec_wind_heat_drought': ('last_day_prec', 'last_day_wind', 'last_day_temp', 'last_day_drought'),\n",
    "    }\n",
    "\n",
    "    for hazard_col, last_day_cols in hazard_pairs.items():\n",
    "        mask = df[\"time\"] > df[list(last_day_cols)].max(axis=1)\n",
    "        df.loc[mask, hazard_col] = np.nan  # Set to NaN if 'time' is later than all corresponding last_day_* columns\n",
    "    return df    \n",
    "    \n",
    "\n",
    "def load_climate():\n",
    "    \"\"\"Reloads climate data with conditions in the real date \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    temp=xr.open_dataset('INPUT_data/temp_data.nc')\n",
    "    drought=xr.open_dataset('INPUT_data/drought_data.nc')\n",
    "    prec=xr.open_dataset('INPUT_data/prec_data.nc')\n",
    "    wind=xr.open_dataset('INPUT_data/wind_data.nc')\n",
    "    df_prec=prec['TOT_PREC'].to_dataframe().reset_index()\n",
    "    df_temp=temp['T_2M'].to_dataframe().reset_index()\n",
    "    df_wind=wind['WIND_SPEED'].to_dataframe().reset_index()\n",
    "    df_drought=drought['SPI_12'].to_dataframe().reset_index()\n",
    "    df_prec['time']=pd.to_datetime(df_prec['time']).dt.date\n",
    "    df_temp['time']=pd.to_datetime(df_temp['time']).dt.date\n",
    "    df_wind['time']=pd.to_datetime(df_wind['time']).dt.date\n",
    "    df_drought['time']=pd.to_datetime(df_drought['time']).dt.date\n",
    "    df_tot=df_prec.merge(df_temp,on=['lon','lat','time'],how='outer')\n",
    "    df_tot=df_tot.merge(df_wind,on=['lon','lat','time'],how='outer')\n",
    "    df_tot=df_tot.merge(df_drought,on=['lon','lat','time'],how='outer')\n",
    "    return(df_tot)\n",
    "\n",
    "def pairing_climate(df_hazards,df_climate):\n",
    "    df_hazards.drop(columns=['TOT_PREC','T_2M','WIND_SPEED','SPI_12','TOT_PREC_MASK','T_2M_MASK','WIND_SPEED_MASK','SPI_12_MASK'],inplace=True)\n",
    "    df_hazards['time']=pd.to_datetime(df_hazards['time']).dt.date\n",
    "    df_hazards=df_hazards.merge(df_climate,on=['time','lon','lat'])\n",
    "    return(df_hazards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e074b6-485c-47fb-9771-8c03e5f0b556",
   "metadata": {},
   "source": [
    "### Graphs and figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037897f-621c-4ba5-a805-731e7b0dfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931730cd-5892-4aba-b848-d6944b4cc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_hazards_compound(df,shapefile, scenario):\n",
    "    import xarray as xr\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    import geopandas as gpd\n",
    "    import geopandas as gpd\n",
    "    from cartopy.io.shapereader import Reader\n",
    "    from cartopy.feature import ShapelyFeature\n",
    "    # Define colors for each class\n",
    "    class_colors = {\n",
    "        'prec_wind': 'navy',\n",
    "        'prec_drought': 'forestgreen',\n",
    "        'prec_heat': 'orchid',\n",
    "        'drought_wind': 'gold',\n",
    "        'drought_heat': 'crimson',\n",
    "        'heat_wind': 'coral',\n",
    "        'prec_wind_heat': 'pink',\n",
    "        'prec_wind_drought': 'cyan',\n",
    "        'prec_drought_heat': 'yellow',\n",
    "        'drought_wind_heat': 'magenta',\n",
    "        'prec_wind_heat_drought': 'black',\n",
    "        'no_hazards':'white'\n",
    "}\n",
    "\n",
    "\n",
    "    #shapefile = 'Limiti/Regioni/Reg01012024_g_WGS84.shp'\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(12.5, 9.6))\n",
    "    # Add borders and features\n",
    "    #ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    #ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    #ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    #ax.add_feature(cfeature.RIVERS)\n",
    "    # Add Veneto borders\n",
    "    gdf.boundary.plot(ax=ax, edgecolor='black', linewidth=0.5, linestyle='-',transform=ccrs.PlateCarree())\n",
    "    # Set the extent to focus on Northeast Italy\n",
    "    ax.set_extent([10.3, 13.3, 44.4, 47.1], crs=ccrs.PlateCarree())\n",
    "    # Add gridlines and labels\n",
    "    gridlines = ax.gridlines(draw_labels=True, linestyle='--')\n",
    "    gridlines.top_labels = False\n",
    "    gridlines.right_labels = False\n",
    "\n",
    "    # Plot each class with a different color\n",
    "    for class_name, color in class_colors.items():\n",
    "        class_data = df[df['class'] == class_name]\n",
    "        ax.scatter(class_data['lon'], class_data['lat'], c=color, label=class_name, s=3)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n",
    "\n",
    "    # Set title\n",
    "    plt.title('Predominant Class by Location '+scenario)\n",
    "    plt.savefig('OUTPUT/'+scenario+'.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ba4ad-34bd-4b11-b200-c1aac84d0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_events_df,flat_mh,flat_sh,flat_mh_only=multi_hazarding(precipitation,wind,heatwave,drought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4744359-df52-44ac-b181-ddf82cea0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is only for testing purposes: it runs on only 5 years of data\n",
    "# to have a robust ana;ysis of multi-hazard combinations at least 30 years fo data are needed\n",
    "\n",
    "plot_multi_hazards_compound(flat_mh_only,'INPUT_data/Limiti/Regioni/Reg01012024_g_WGS84.shp','2018-2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12be783-09b7-47c6-ac07-56e615aa048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is working with the new precipitation, wind, heatwave and drought dataframes, \n",
    "# with renamed columns for the identified clusters after the compound event analysis\n",
    "time_lag7=consecutive_hazards(precipitation,wind,heatwave,drought,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3594627-c5bd-4396-a8f0-ebce4352542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_lag7.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88949d6a-1756-46b8-84c9-4f11c742e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate=load_climate()\n",
    "consecutive_events_tl7=pairing_climate(time_lag7,df_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e6a24-31f3-451d-b8a2-041a14789d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_events_tl7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bf77e-8571-423f-8f48-235154cb450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_events_tl7.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22537e45-e3f2-4b8b-913e-4a39e2128c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_mh_tl7,flat_sh_tl7,flat_mh_tl7=compound_events_flattening(consecutive_events_tl7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c7b64-be1b-4424-a6e2-d5a473608087",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_mh_tl7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b7776-9edf-4079-b95e-7b7f844c6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is only for testing purposes: it runs on only 5 years of data\n",
    "# to have a robust ana;ysis of multi-hazard combinations at least 30 years fo data are needed\n",
    "\n",
    "plot_multi_hazards_compound(flat_mh_tl7,'INPUT_data/Limiti/Regioni/Reg01012024_g_WGS84.shp','2018-2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0872e-19dd-4360-9578-23b03230bfde",
   "metadata": {},
   "source": [
    "# Attributes for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aef67d-46b7-4ba2-8052-e7e0a5601b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def filter_veneto(df, shapefile_path, name_column='tipo_p'):\n",
    "    \"\"\"\n",
    "    Filters the input DataFrame to only include points within the Veneto region,\n",
    "    and adds a column with landscape name.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame with 'lon' and 'lat' columns\n",
    "    - shapefile_path: path to the Veneto shapefile (should be in EPSG:4326 or reprojected)\n",
    "    - name_column: name of the column in shapefile for province/landscape (e.g., 'tipo_p')\n",
    "\n",
    "    Returns:\n",
    "    - filtered DataFrame with a new column (landscape) for the landscape info within the Veneto Region\n",
    "    \"\"\"\n",
    "    # Load shapefile as GeoDataFrame\n",
    "    veneto_gdf = gpd.read_file(shapefile_path)\n",
    "    veneto_gdf = veneto_gdf.to_crs(epsg=4326)  # Ensure CRS is WGS84\n",
    "\n",
    "    # Create GeoDataFrame from input df\n",
    "    geometry = gpd.points_from_xy(df['lon'], df['lat'])\n",
    "    df_gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "    # Spatial join: match points to polygons\n",
    "    joined = gpd.sjoin(df_gdf, veneto_gdf[[name_column, 'geometry']], how='inner', predicate='within')\n",
    "\n",
    "    # Rename the joined name column to avoid confusion\n",
    "    joined = joined.rename(columns={name_column: 'landscape'})\n",
    "\n",
    "    return joined.drop(columns=['geometry','index_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5173c-5768-4242-b12a-3c2df776ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_veneto(precipitation,'INPUT_data/boundaries/c1103018013_ricognizionepaesaggivenetoperimetri.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b97377-52fd-4b4f-a673-1fd6901143f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation.prec_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6500b6-c1fc-4f35-8006-7bd9f33c8ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_extension(df,cluster_name,name):\n",
    "    \"\"\"\n",
    "    Calculates the total extension (in days) of each cluster at each location (lon, lat),\n",
    "    based on the number of days with a valid TOT_PREC_MASK value.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing at least ['lon', 'lat', cluster_name, name]\n",
    "    - cluster_name: str, name of the column representing the ID of the cluster/group (e.g., prec_clusters)\n",
    "    - name: str, name of the column representing the variable of the cluster (e.g., TOT_PREC)\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with [cluster_name, 'total_extension']\n",
    "    \"\"\"\n",
    "    total_extension_df=df.groupby([cluster_name])[name].size().reset_index(name='total_extension')\n",
    "    return(total_extension_df)\n",
    "\n",
    "def duration(df, cluster_name, date_column='time'):\n",
    "    \"\"\"\n",
    "    Calculates duration per cluster based on min and max date.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with at least [cluster_name, date_column]\n",
    "    - cluster_name: str, column name identifying clusters (e.g., 'landscape')\n",
    "    - date_column: str, name of the date column (default: 'date')\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with cluster_name, min_date, max_date, and duration (days)\n",
    "    \"\"\"\n",
    "    # Ensure date column is datetime\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    result = (\n",
    "        df.groupby(cluster_name)[date_column]\n",
    "          .agg(min_date='min', max_date='max')\n",
    "          .reset_index()\n",
    "    )\n",
    "    \n",
    "    result['duration'] = (result['max_date'] - result['min_date']).dt.days + 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def spatial_extension(df, cluster_name):\n",
    "    \"\"\"\n",
    "    Calculates the number of unique (lon, lat) cells involved in each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing ['lon', 'lat', cluster_name]\n",
    "    - cluster_name: str, column name representing the cluster ID\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with [cluster_name, 'total_cells']\n",
    "    \"\"\"\n",
    "    total_cells_df = (\n",
    "        df.drop_duplicates(subset=['lon', 'lat', cluster_name])  # ensure unique spatial points per cluster\n",
    "          .groupby(cluster_name)[['lon', 'lat']]\n",
    "          .size()\n",
    "          .reset_index(name='total_cells')\n",
    "    )\n",
    "    return total_cells_df\n",
    "\n",
    "def mean_intensity_per_cluster(df, cluster_name, intensity_var):\n",
    "    \"\"\"\n",
    "    Calculates the mean intensity of a given variable for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing at least [cluster_name, intensity_var]\n",
    "    - cluster_name: str, name of the column representing the cluster ID\n",
    "    - intensity_var: str, name of the column representing intensity (e.g., SPI_12)\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with [cluster_name, 'mean_intensity']\n",
    "    \"\"\"\n",
    "    mean_intensity_df = (\n",
    "        df.groupby(cluster_name)[intensity_var]\n",
    "        .mean()\n",
    "        .reset_index(name='mean_intensity')\n",
    "    )\n",
    "    return mean_intensity_df\n",
    "\n",
    "def max_intensity_per_cluster(df, cluster_name, intensity_var):\n",
    "    \"\"\"\n",
    "    Calculates the max intensity of a given variable for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing at least [cluster_name, intensity_var]\n",
    "    - cluster_name: str, name of the column representing the cluster ID\n",
    "    - intensity_var: str, name of the column representing intensity (e.g., SPI_12)\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with [cluster_name, 'mean_intensity']\n",
    "    \"\"\"\n",
    "    if cluster_name=='drought_clusters':\n",
    "        max_intensity_df = (\n",
    "            df.groupby(cluster_name)[intensity_var]\n",
    "            .min()\n",
    "            .reset_index(name='mean_intensity')\n",
    "        )\n",
    "    else:\n",
    "        max_intensity_df = (\n",
    "                df.groupby(cluster_name)[intensity_var]\n",
    "                .max()\n",
    "                .reset_index(name='mean_intensity')\n",
    "            )\n",
    "    return max_intensity_df\n",
    "\n",
    "def seasonality_per_cluster(df, cluster_name):\n",
    "    \"\"\"\n",
    "    Calculates the dominant season for each cluster (i.e., the season with the majority of occurrences).\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with at least ['time', cluster_name]\n",
    "    - cluster_name: str, name of the column representing the cluster ID\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with [cluster_name, 'seasonality']\n",
    "    \"\"\"\n",
    "    # Extract month from time column\n",
    "    df = df.copy()\n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month\n",
    "\n",
    "    # Map months to seasons\n",
    "    month_to_season = {\n",
    "        12: 'winter', 1: 'winter', 2: 'winter',\n",
    "        3: 'spring', 4: 'spring', 5: 'spring',\n",
    "        6: 'summer', 7: 'summer', 8: 'summer',\n",
    "        9: 'fall', 10: 'fall', 11: 'fall'\n",
    "    }\n",
    "    df['season'] = df['month'].map(month_to_season)\n",
    "\n",
    "    # Get dominant season per cluster (mode of season values)\n",
    "    seasonality_df = (\n",
    "        df.groupby(cluster_name)['season']\n",
    "        .agg(lambda x: x.mode().iloc[0])  # Use mode (most frequent)\n",
    "        .reset_index(name='seasonality')\n",
    "    )\n",
    "\n",
    "    return seasonality_df\n",
    "\n",
    "def landscape_per_cluster(df, cluster_name):\n",
    "    \"\"\"\n",
    "    Calculates the dominant (most frequent) landscape type for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with at least [cluster_name, 'landscape']\n",
    "    - cluster_name: str, the column name representing the cluster ID\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with [cluster_name, 'dominant_landscape']\n",
    "    \"\"\"\n",
    "    df=filter_veneto(df,'boundaries/c1103018013_ricognizionepaesaggivenetoperimetri.shp')\n",
    "\n",
    "    dominant_landscape_df = (\n",
    "        df.groupby(cluster_name)['landscape']\n",
    "        .agg(lambda x: x.mode().iloc[0])  # Use most frequent landscape\n",
    "        .reset_index(name='dominant_landscape')\n",
    "    )\n",
    "    \n",
    "    return dominant_landscape_df\n",
    "\n",
    "    return seasonality_df\n",
    "\n",
    "def compound_event_stats(df, cluster_name, hazard1, hazard2):\n",
    "    \"\"\"\n",
    "    Calculates mean and max intensity for two hazards, and total extension for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with at least [cluster_name, hazard1, hazard2]\n",
    "    - cluster_name: str, column name of cluster ID\n",
    "    - hazard1: str, first hazard intensity column (e.g., 'SPI_12')\n",
    "    - hazard2: str, second hazard intensity column (e.g., 'TOT_PREC')\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with [cluster_name, mean/max of hazard1/2, total_extension]\n",
    "    \"\"\"\n",
    "\n",
    "   # Get hazard names from cluster_name string\n",
    "    try:\n",
    "        name1, name2 = cluster_name.split('_')\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Expected cluster_name to contain exactly one '_', got: {cluster_name}\")\n",
    "\n",
    "    is_drought = 'drought' in cluster_name.lower()\n",
    "\n",
    "    grouped = df.groupby(cluster_name)\n",
    "\n",
    "    result = grouped.agg(\n",
    "        **{f'mean_{name1}': (hazard1, 'mean')},\n",
    "        **{f'max_{name1}': (hazard1, lambda x: x.min() if is_drought else x.max())},\n",
    "        **{f'mean_{name2}': (hazard2, 'mean')},\n",
    "        **{f'max_{name2}': (hazard2, lambda x: x.min() if is_drought else x.max())},\n",
    "        total_extension=(hazard1, 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    return result\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_compound_event_stats(df, hazard1_label, hazard2_label, label_dict=None):\n",
    "    \"\"\"\n",
    "    Scatter plot of mean intensities of two hazards.\n",
    "    Point size proportional to total extension.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with columns like mean_{hazard1_label}, mean_{hazard2_label}, total_extension\n",
    "    - hazard1_label: str, key for first hazard (used to find mean_{hazard1_label})\n",
    "    - hazard2_label: str, key for second hazard (used to find mean_{hazard2_label})\n",
    "    - label_dict: dict mapping hazard keys to axis labels (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    col1 = f\"mean_{hazard1_label}\"\n",
    "    col2 = f\"mean_{hazard2_label}\"\n",
    "\n",
    "    if col1 not in df.columns or col2 not in df.columns:\n",
    "        raise ValueError(f\"DataFrame must contain columns '{col1}' and '{col2}'\")\n",
    "\n",
    "    xlabel = label_dict.get(hazard1_label, hazard1_label) if label_dict else hazard1_label\n",
    "    ylabel = label_dict.get(hazard2_label, hazard2_label) if label_dict else hazard2_label\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    scatter = plt.scatter(\n",
    "        df[col1], df[col2],\n",
    "        s=df['total_extension'] * 2,\n",
    "        alpha=0.7, edgecolors='black'\n",
    "    )\n",
    "\n",
    "    plt.xlabel(f'Mean Intensity: {xlabel}')\n",
    "    plt.ylabel(f'Mean Intensity: {ylabel}')\n",
    "    plt.title('Compound Events: Mean Intensities vs. Extension')\n",
    "\n",
    "    # Invert axis if drought is involved\n",
    "    if hazard1_label == 'drought':\n",
    "        plt.gca().invert_xaxis()\n",
    "    if hazard2_label == 'drought':\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a7bc4-9543-4738-abd4-6154619afb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_events_df_veneto=filter_veneto(compound_events_df,'INPUT_data/boundaries/c1103018013_ricognizionepaesaggivenetoperimetri.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5bd2c-6977-4bcd-8fcb-6d6fbbdea149",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_heat_df=compound_event_stats(compound_events_df_veneto, 'drought_heat', 'SPI_12','T_2M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac1fd6-52ac-4690-ac6d-0ebc6e944be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_wind_df=compound_event_stats(compound_events_df_veneto, 'prec_wind', 'TOT_PREC','WIND_SPEED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb90e36-5290-4925-932e-0b3a547521f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_heat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a3c1a-7ee6-4d66-aacc-297964a4a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_wind_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ff330-6763-4d6b-bb9e-7c57cb053ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plot is only for testing: to have robust result, at least 30 years of data are required\n",
    "plot_compound_event_stats(prec_wind_df, \"prec\", \"wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbd07d-0b74-4891-b504-4628828bf1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plot is only for testing: to have robust result, at least 30 years of data are required\n",
    "plot_compound_event_stats(drought_heat_df, \"drought\", \"heat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
